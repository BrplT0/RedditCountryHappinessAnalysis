# ğŸŒ Reddit Country Happiness Analysis

> A large-scale NLP project analyzing sentiment patterns across European countries through Reddit comments to create an interactive happiness visualization dashboard.

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PRAW](https://img.shields.io/badge/PRAW-7.7+-orange.svg)](https://praw.readthedocs.io/)
[![Pandas](https://img.shields.io/badge/Pandas-2.0+-green.svg)](https://pandas.pydata.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“Š Project Overview

This project leverages Reddit's vast user-generated content to analyze and visualize happiness trends across European nations. By processing **1 million+ comments** from country-specific subreddits, we aim to create a sentiment-based heatmap showing which regions are happiest based on social media discourse.

> âš ï¸ **Note**: This is my first major data engineering project. While functional, there may be areas for optimization and improvement. Feedback and contributions are welcome!

### Current Capabilities

- ğŸ—ºï¸ **150+ Countries Supported** - Scalable to global analysis
- ğŸ‡ªğŸ‡º **European Focus** - Majority of European countries covered
- ğŸ’¬ **500K+ Comments Collected** - Processed in ~2 hours
- ğŸ“¥ **High-Speed Scraping** - ~300K comments/hour (optimizing further)
- ğŸ”„ **Weekly Automation Ready** - Self-updating data pipeline
- ğŸ¤– **Multi-lingual Support** - Handles 20+ European languages

---

## ğŸ¯ Motivation

Social media platforms contain authentic, unfiltered opinions about daily life, politics, economy, and social issues. By analyzing these conversations at scale, we can:

- Track happiness trends over time
- Compare sentiment across cultures
- Identify regional events' emotional impact
- Provide data-driven insights for researchers and policymakers

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Reddit API (PRAW)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Subreddit Checker  â”‚  (197 countries validated)
      â”‚                     â”‚  (Changeable approval: 100K+ subs, 1000+ comments/week)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Post Scraper     â”‚  (Last 7 days, approved subreddits)
      â”‚                     â”‚  (Collects post metadata)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Comment Scraper    â”‚  âœ… COMPLETE
      â”‚                     â”‚  (500K+ comments)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Preprocessing     â”‚  â³ IN PROGRESS
      â”‚                     â”‚  (Spam filter, data cleaning)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Sentiment Analysis  â”‚  ğŸ”œ PLANNED
      â”‚                     â”‚  (Multi-lingual NLP)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Data Aggregation   â”‚  ğŸ”œ PLANNED
      â”‚                     â”‚  (Country-level statistics)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Streamlit Dashboardâ”‚  ğŸ”œ PLANNED
      â”‚                     â”‚  (Interactive choropleth map)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.9+** - Core language
- **PRAW 7.7+** - Reddit API wrapper with rate limiting
- **Pandas** - Data manipulation and CSV processing
- **Multiprocessing** - Parallel execution for performance

### Data Collection
- **Custom Scrapers** - Post and comment collection
- **Approval System** - Quality filtering (subscriber/activity thresholds)
- **Logging System** - Comprehensive activity tracking

### Planned NLP & Visualization
- **TextBlob** / **XLM-RoBERTa** - Sentiment analysis (evaluating options)
- **Streamlit** - Interactive web dashboard
- **Plotly** - Choropleth map visualization

### Infrastructure
- **ConfigParser** - Configuration management
- **Pathlib** - Cross-platform path handling
- **Custom Logger** - File and console logging

---

## ğŸ“‚ Project Structure

```
RedditCountryHappinessAnalysis/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analyzers/              # ğŸ”œ NLP and aggregation
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.py
â”‚   â”‚   â””â”€â”€ data_aggregator.py
â”‚   â”œâ”€â”€ checkers/               # âœ… Subreddit validation
â”‚   â”‚   â”œâ”€â”€ check_subreddits.py
â”‚   â”‚   â””â”€â”€ filter_subreddits.py
â”‚   â”œâ”€â”€ core/                   # âœ… Core utilities
â”‚   â”‚   â”œâ”€â”€ connect_reddit.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ config_utils.py
â”‚   â”œâ”€â”€ scrapers/               # âœ… Data collection
â”‚   â”‚   â”œâ”€â”€ subreddit_scraper.py
â”‚   â”‚   â””â”€â”€ comment_scraper.py
â”‚   â”œâ”€â”€ utils/                  # âœ… Helper functions
â”‚   â”‚   â”œâ”€â”€ save_csv.py
â”‚   â”‚   â””â”€â”€ subreddit_stats.py
â”‚   â””â”€â”€ dashboard/              # ğŸ”œ Visualization
â”‚       â””â”€â”€ app.py
â”œâ”€â”€ main.py                     # Main execution pipeline
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

> **âš ï¸ Important:** `/data` directory is excluded from version control for privacy and data size reasons (contains 1M+ comments). The `.env` file with API credentials is also not included. When forking, you'll need to:
> 1. Set up your own Reddit API credentials in `.env`
> 2. Create the data directory structure manually
> 3. Adjust `config/config.ini` parameters to your needs
>
> This may cause initial setup challenges - this is my first major project and I'm still learning best practices for repository management.

---

## âš™ï¸ Setup & Installation

> **Note:** Due to missing `/data` directory and `.env` file, this repository is primarily for showcasing the codebase. Running locally requires additional setup.

### Prerequisites
- Python 3.9 or higher
- Reddit API credentials ([Get them here](https://www.reddit.com/prefs/apps))
- 6GB+ RAM recommended for large-scale processing

### Installation

```bash
# Clone the repository
git clone https://github.com/BrplT0/RedditCountryHappinessAnalysis.git
cd RedditCountryHappinessAnalysis

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

**1. Reddit API Credentials** - Create `.env` in project root:
```env
CLIENT_ID=your_reddit_client_id
CLIENT_SECRET=your_reddit_client_secret
```

**2. Data Directory** - Create the following structure:
```
data/
â”œâ”€â”€ archived/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.ini
â”œâ”€â”€ dashboard/
â”œâ”€â”€ logs/
â”œâ”€â”€ processed/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ subreddits/
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ weekly_scrapings/
â”‚   â”‚   â”œâ”€â”€ comments/
â”‚   â”‚   â””â”€â”€ posts/
â”œâ”€â”€ processed/
â””â”€â”€ logs/
```

**3. Config Settings** - Adjust `config/config.ini` parameters if needed:
```ini
[global]

#scrapig category of subreddits
#parameters : "all", "world", "asia", "africa", "europe", "south_america", "north_america", "oceania"
category = europe

#Scrapes till date - comment_max_days and stops scrapping, default = 7 (one week).
comment_max_days = 7

[check_subreddits]

# If comment count is lower than or equal to this value, the subreddit will be approved and scraping stops (default = 1000).
comment_approve_point = 1000

#If subscriber count is higher than this, subreddit will be approved, default = 100000.
sub_approve_point = 100000

[reddit_post_scraper]

#declare how many post do you want to scrape, default = 750.
post_limit = 750

# If comment count in the post is equals or higher than this, post will be approved, default = 50.
post_comment_approve_limit = 50

[reddit_comment_scraper]

comment_link_limit = 2
```

---

## ğŸš€ Current Status & Usage

### Completed Modules âœ…

**1. Subreddit Validation**
```bash
python -m src.checkers.check_subreddits
```
- Validates 197 country subreddits
- Checks subscriber count (>20K threshold)
- Analyzes activity (>50 comments/week)
- Outputs approved subreddits for scraping

**2. Post Collection**
```bash
python -m src.scrapers.subreddit_scraper
```
- Scrapes posts from approved subreddits
- Collects last 7 days of content
- Stores post metadata (title, score, comments count)

**3. Comment Collection**
```bash
python -m src.scrapers.comment_scraper
```
- Post ID-based comment extraction
- Nested thread support
- **Performance: 500K comments in ~2 hours**

### Full Pipeline (Current State)

```bash
python main.py
```

Executes:
1. âœ… Subreddit validation
2. âœ… Post scraping
3. âœ… Comment collection
4. â³ Data preprocessing (in development)

---

## ğŸ“ˆ Performance Metrics

| Metric | Current Performance |
|--------|---------------------|
| **Countries Supported** | 150+ (Global capability) |
| **Active Focus** | Majority of European countries |
| **Comments Collected** | 1,000,000+ |
| **Scraping Time** | ~2 hours |
| **Average Speed** | ~500,000 comments/hour |
| **Approved Subreddits** | ~40-60 (varies by region/week) |

> **Optimization Note:** Currently working on improving scraping speed through better batching and parallel processing.

---

## ğŸ§  Planned Sentiment Analysis

### Data Cleaning Strategy
- Remove `[deleted]` and `[removed]` comments
- Filter bot accounts and AutoModerator
- Eliminate comments < 10 characters
- Handle special characters and emojis

### Multi-lingual NLP Options (Under Evaluation)

**Option A: TextBlob**
- Fast processing (~10K comments/min)
- Basic accuracy, good for prototyping
- Works across multiple languages

**Option B: XLM-RoBERTa**
- High accuracy for 100+ languages
- Requires GPU for reasonable speed
- Pre-trained on social media text

**Option C: Language-Specific Models**
- Best accuracy per language
- Complex pipeline management
- Higher maintenance overhead

### Sentiment Scoring System (Planned)

| Score | Category | Emoji |
|-------|----------|-------|
| 0.5 to 1.0 | Very Positive | ğŸ˜„ |
| 0.05 to 0.5 | Positive | ğŸ™‚ |
| -0.05 to 0.05 | Neutral | ğŸ˜ |
| -0.5 to -0.05 | Negative | ğŸ˜Ÿ |
| -1.0 to -0.5 | Very Negative | ğŸ˜¢ |

---

## ğŸ“Š Planned Dashboard Features

### Interactive Visualizations
- **Choropleth World Map**: Countries colored by happiness score
- **Time Series Analysis**: Weekly/monthly trend tracking
- **Country Rankings**: Top happiest/unhappiest nations
- **Sentiment Distribution**: Pie charts showing positive/neutral/negative ratios
- **Word Clouds**: Most discussed topics per country

### Technologies
- **Streamlit**: Rapid dashboard prototyping
- **Plotly**: Interactive maps and charts
- **Pandas**: Data aggregation and filtering

---

## ğŸ“ Key Learnings & Challenges

### Technical Achievements
- âœ… Handled 500K+ records efficiently with memory-conscious design
- âœ… Optimized Reddit API usage within rate limits (60 requests/min)
- âœ… Built modular, maintainable codebase with separation of concerns
- âœ… Implemented robust logging and error handling

### Challenges Encountered
- **Rate Limiting**: Learned to batch requests and implement smart delays
- **Path Management**: Struggled with relative vs absolute paths; resolved with pathlib
- **Data Volume**: Initial memory issues; solved with streaming writes and batch processing
- **Multi-stage Pipeline**: Coordinating multiple scripts; created main.py orchestrator

### Skills Developed
- Large-scale data collection and ETL pipelines
- API optimization and rate limit management
- Python project architecture and modularity
- Git version control and documentation
- Problem-solving under constraints (API limits, memory)

### Areas for Improvement
- More efficient data structures for faster processing
- Better exception handling in edge cases
- Unit tests for critical functions
- Performance profiling and optimization

---

## ğŸ”® Roadmap

### Short-term (Next Steps)
- [ ] Complete data preprocessing module
- [ ] Implement sentiment analysis (choosing between TextBlob/XLM-RoBERTa)
- [ ] Build data aggregation pipeline
- [ ] Create basic Streamlit dashboard

### Medium-term
- [ ] Expand beyond Europe to global coverage
- [ ] Optimize scraping speed (target: <1 hour for 500K comments)
- [ ] Add time series analysis
- [ ] Implement automated weekly updates

### Long-term (Future Enhancements)
- [ ] Real-time streaming dashboard
- [ ] Correlation analysis with geopolitical events
- [ ] API endpoint for third-party access
- [ ] Mobile-responsive interface
- [ ] Predictive modeling for happiness trends

---

## ğŸ› Known Issues & Limitations

- `/data` directory and `.env` not included in repository (requires manual setup)
- No automated testing suite yet
- Dashboard not yet implemented
- Limited to subreddits with sufficient activity (20K+ subscribers)
- Some smaller countries may lack Reddit presence
- Performance bottleneck in single-threaded sections

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Berat Polat**

- GitHub: [@BrplT0](https://github.com/BrplT0)
- LinkedIn: [Berat Polat](https://www.linkedin.com/in/berat-polat-923093249)
- Email: beratpolat0402@gmail.com

---

## ğŸ™ Acknowledgments

- **Reddit & PRAW**: For comprehensive API access and excellent documentation
- **HuggingFace**: For pre-trained NLP models and transformers library
- **Streamlit Community**: For making data visualization accessible

---

## ğŸ“ Contact & Contributions

This is my first major data project, and I'm learning as I build. Feedback, suggestions, and contributions are highly appreciated!

- **Issues**: [GitHub Issues](https://github.com/BrplT0/RedditCountryHappinessAnalysis/issues)
- **Pull Requests**: Welcome! Please open an issue first to discuss major changes
- **Email**: beratpolat0402@gmail.com

---

<div align="center">

### â­ If you find this project interesting, please consider starring it!

**Status**: ğŸš§ Active Development | ğŸ“Š Data Collection Complete | ğŸ¤– NLP In Progress

Built with â¤ï¸ and â˜• | Learning by doing

</div>
