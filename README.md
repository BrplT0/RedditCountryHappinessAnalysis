# üåç Reddit Country Happiness Analysis

> A large-scale NLP project analyzing sentiment patterns across European countries through Reddit comments to create an interactive happiness visualization dashboard.

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PRAW](https://img.shields.io/badge/PRAW-7.7+-orange.svg)](https://praw.readthedocs.io/)
[![Pandas](https://img.shields.io/badge/Pandas-2.0+-green.svg)](https://pandas.pydata.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## üìä Project Overview

This project leverages Reddit's vast user-generated content to analyze and visualize happiness trends across European nations. By processing **1 million+ comments** from country-specific subreddits, we aim to create a sentiment-based heatmap showing which regions are happiest based on social media discourse.

> ‚ö†Ô∏è **Note**: This is my first major data engineering project. While functional, there may be areas for optimization and improvement. Feedback and contributions are welcome!

### Current Capabilities

- üó∫Ô∏è **150+ Countries Supported** - Scalable to global analysis
- üá™üá∫ **European Focus** - Majority of European countries covered
- üí¨ **1M+ Comments Collected** - Processed efficiently
- üì• **High-Speed Scraping** - ~400K comments/hour (optimized)
- üîÑ **Weekly Automation Ready** - Self-updating data pipeline
- ü§ñ **Multi-lingual Support** - Handles 20+ European languages

---

## üéØ Motivation

Social media platforms contain authentic, unfiltered opinions about daily life, politics, economy, and social issues. By analyzing these conversations at scale, we can:

- Track happiness trends over time
- Compare sentiment across cultures
- Identify regional events' emotional impact
- Provide data-driven insights for researchers and policymakers

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Reddit API (PRAW)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Subreddit Checker  ‚îÇ  (197 countries validated)
      ‚îÇ                     ‚îÇ  (Threshold: 100K+ subs, 1000+ comments/week)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ    Post Scraper     ‚îÇ  (Last 7 days, approved subreddits)
      ‚îÇ                     ‚îÇ  (Collects post metadata)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Comment Scraper    ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (1M+ comments, 30min for 200K)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ   Preprocessing     ‚îÇ  ‚è≥ IN PROGRESS
      ‚îÇ                     ‚îÇ  (Spam filter, data cleaning)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Sentiment Analysis  ‚îÇ  üîú PLANNED
      ‚îÇ                     ‚îÇ  (Multi-lingual NLP)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Data Aggregation   ‚îÇ  üîú PLANNED
      ‚îÇ                     ‚îÇ  (Country-level statistics)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Streamlit Dashboard‚îÇ  üîú PLANNED
      ‚îÇ                     ‚îÇ  (Interactive choropleth map)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

### Backend
- **Python 3.9+** - Core language
- **PRAW 7.7+** - Reddit API wrapper with rate limiting
- **Pandas** - Data manipulation and CSV processing
- **Multiprocessing** - Parallel execution for performance
- **Dotenv** - Environment variable management for secrets

### Data Collection
- **Custom Scrapers** - Post and comment collection
- **Approval System** - Quality filtering (subscriber/activity thresholds)
- **Logging System** - Comprehensive activity tracking

### Planned NLP & Visualization
- **TextBlob** / **XLM-RoBERTa** - Sentiment analysis (evaluating options)
- **Streamlit** - Interactive web dashboard
- **Plotly** - Choropleth map visualization

### Infrastructure
- **ConfigParser** - Configuration management
- **Pathlib** - Cross-platform path handling
- **Custom Logger** - File and console logging

---

## üìÇ Project Structure

This project follows a standardized architecture separating configuration, assets, source code, and data.

```
RedditCountryHappinessAnalysis/
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ subreddits.csv            # üîí Populated list (Ignored by Git)
‚îÇ   ‚îî‚îÄ‚îÄ subreddits.template.csv   # ‚úÖ Public template (Tracked by Git)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.ini                # ‚úÖ Public settings (Tracked by Git)
‚îú‚îÄ‚îÄ data/                          # üîí Generated outputs (Ignored by Git)
‚îÇ   ‚îú‚îÄ‚îÄ archived/
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ weekly_scrapings/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ analyzers/                # üîú NLP and aggregation
‚îÇ   ‚îú‚îÄ‚îÄ checkers/                 # ‚úÖ Subreddit validation
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # ‚úÖ Core utilities
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/                 # ‚úÖ Data collection
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # ‚úÖ Helper functions
‚îÇ   ‚îî‚îÄ‚îÄ dashboard/                # üîú Visualization
‚îú‚îÄ‚îÄ .env                          # üîí API secrets (Ignored by Git)
‚îú‚îÄ‚îÄ main.py                       # Main execution pipeline
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

> **Note on Project Setup:**
>
> This repository is configured for easy setup, but requires manual folder creation.
> - The `/data` directory is **entirely ignored by Git**. You **must** create the `/data` folder and its subdirectories (`raw`, `logs`, `processed`, etc.) manually for the scripts to run.
> - You **must** manually create the `.env` file for your API keys.
> - You **must** manually create the `assets/subreddits.csv` file from the provided template.

---

## ‚öôÔ∏è Setup & Installation

> **Note:** Running locally requires you to provide your own API credentials, subreddit list, and data folder structure.

### Prerequisites
- Python 3.9 or higher
- Reddit API credentials ([Get them here](https://www.reddit.com/prefs/apps))
- 6GB+ RAM recommended for large-scale processing

### Installation

```bash
# Clone the repository
git clone https://github.com/BrplT0/RedditCountryHappinessAnalysis.git
cd RedditCountryHappinessAnalysis

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration (Required Steps)

**1. Reddit API Credentials (Secrets)**

Create a `.env` file in the project root:

```env
CLIENT_ID=your_reddit_client_id
CLIENT_SECRET=your_reddit_client_secret
```

**2. Subreddit List (Input Data)**

The code needs `assets/subreddits.csv` to run. Create this file by duplicating the template:

```bash
# On Linux/macOS
cp assets/subreddits.template.csv assets/subreddits.csv

# On Windows
copy assets\subreddits.template.csv assets\subreddits.csv
```

Now, **edit `assets/subreddits.csv`** and populate it with the subreddits you want to analyze.

**3. Data Directory (Critical Step)**

You **must** manually create the `data/` folder structure that the scripts expect. Create these folders in the project root:

```bash
mkdir -p data/archived
mkdir -p data/dashboard
mkdir -p data/logs
mkdir -p data/processed
mkdir -p data/raw/subreddits
mkdir -p data/weekly_scrapings/comments
mkdir -p data/weekly_scrapings/posts
```

**4. General Settings (Public)**

Adjust parameters in `config/config.ini` as needed. The defaults are sensible:

```ini
[global]
# Scraping category of subreddits
# Parameters: "all", "world", "asia", "africa", "europe", "south_america", "north_america", "oceania"
category = europe

# Scrapes till date - comment_max_days and stops scraping, default = 7 (one week)
comment_max_days = 7

[check_subreddits]
# ... (and other settings) ...
```

---

## üöÄ Current Status & Usage

### Completed Modules ‚úÖ

**1. Subreddit Validation**

```bash
python -m src.checkers.check_subreddits
```

- Validates 197 country subreddits
- Checks subscriber count (>20K threshold)
- Analyzes activity (>50 comments/week)
- Outputs approved subreddits for scraping

**2. Post Collection**

```bash
python -m src.scrapers.subreddit_scraper
```

- Scrapes posts from approved subreddits
- Collects last 7 days of content
- Stores post metadata (title, score, comments count)

**3. Comment Collection**

```bash
python -m src.scrapers.comment_scraper
```

- Post ID-based comment extraction
- Nested thread support
- **Performance: 200K comments in ~30 minutes** (~400K/hour)

### Full Pipeline (Current State)

```bash
python main.py
```

Executes:
1. ‚úÖ Subreddit validation
2. ‚úÖ Post scraping
3. ‚úÖ Comment collection
4. ‚è≥ Data preprocessing (in development)

---

## üìà Performance Metrics

| Metric | Current Performance |
|--------|---------------------|
| **Countries Supported** | 150+ (Global capability) |
| **Active Focus** | Majority of European countries |
| **Comments Collected** | 1,000,000+ |
| **Scraping Time** | ~30 minutes for 200K comments |
| **Average Speed** | ~400,000 comments/hour |
| **Approved Subreddits** | ~40-60 (varies by region/week) |

> **Performance Achievement:** Recently collected 200K comments in just 30 minutes through optimized batching and parallel processing!

---

## üß† Planned Sentiment Analysis

### Data Cleaning Strategy

- Remove `[deleted]` and `[removed]` comments
- Filter bot accounts and AutoModerator
- Eliminate comments < 10 characters
- Handle special characters and emojis

### Multi-lingual NLP Options (Under Evaluation)

**Option A: TextBlob**
- Fast processing (~10K comments/min)
- Basic accuracy, good for prototyping
- Works across multiple languages

**Option B: XLM-RoBERTa**
- High accuracy for 100+ languages
- Requires GPU for reasonable speed
- Pre-trained on social media text

**Option C: Language-Specific Models**
- Best accuracy per language
- Complex pipeline management
- Higher maintenance overhead

### Sentiment Scoring System (Planned)

| Score | Category | Emoji |
|-------|----------|-------|
| 0.5 to 1.0 | Very Positive | üòÑ |
| 0.05 to 0.5 | Positive | üôÇ |
| -0.05 to 0.05 | Neutral | üòê |
| -0.5 to -0.05 | Negative | üòü |
| -1.0 to -0.5 | Very Negative | üò¢ |

---

## üìä Planned Dashboard Features

### Interactive Visualizations

- **Choropleth World Map**: Countries colored by happiness score
- **Time Series Analysis**: Weekly/monthly trend tracking
- **Country Rankings**: Top happiest/unhappiest nations
- **Sentiment Distribution**: Pie charts showing positive/neutral/negative ratios
- **Word Clouds**: Most discussed topics per country

### Technologies

- **Streamlit**: Rapid dashboard prototyping
- **Plotly**: Interactive maps and charts
- **Pandas**: Data aggregation and filtering

---

## üéì Key Learnings & Challenges

### Technical Achievements

- ‚úÖ Handled 500K+ records efficiently with memory-conscious design
- ‚úÖ Optimized Reddit API usage within rate limits (60 requests/min)
- ‚úÖ Built modular, maintainable codebase with separation of concerns
- ‚úÖ Implemented robust logging and error handling

### Challenges Encountered

- **Rate Limiting**: Learned to batch requests and implement smart delays
- **Path Management**: Struggled with relative vs absolute paths; **resolved with pathlib & root-level folders**
- **Data Volume**: Initial memory issues; solved with streaming writes and batch processing
- **Multi-stage Pipeline**: Coordinating multiple scripts; created main.py orchestrator

### Skills Developed

- Large-scale data collection and ETL pipelines
- API optimization and rate limit management
- Python project architecture and modularity
- Git version control and documentation
- Problem-solving under constraints (API limits, memory)

### Areas for Improvement

- More efficient data structures for faster processing
- Better exception handling in edge cases
- Unit tests for critical functions
- Performance profiling and optimization

---

## üîÆ Roadmap

### Short-term (Next Steps)

- [ ] Complete data preprocessing module
- [ ] Implement sentiment analysis (choosing between TextBlob/XLM-RoBERTa)
- [ ] Build data aggregation pipeline
- [ ] Create basic Streamlit dashboard

### Medium-term

- [ ] Expand beyond Europe to global coverage
- [ ] Optimize scraping speed (target: <1 hour for 500K comments)
- [ ] Add time series analysis
- [ ] Implement automated weekly updates

### Long-term (Future Enhancements)

- [ ] Real-time streaming dashboard
- [ ] Correlation analysis with geopolitical events
- [ ] API endpoint for third-party access
- [ ] Mobile-responsive interface
- [ ] Predictive modeling for happiness trends

---

## üêõ Known Issues & Limitations

- **Requires Manual Setup**: `/data` directory, `.env` file (for API keys), and `assets/subreddits.csv` (for input data) must be created manually
- **No Test Suite**: No automated testing suite yet
- **Dashboard Pending**: Dashboard not yet implemented
- **Data Scarcity**: Limited to subreddits with sufficient activity (20K+ subscribers). Some smaller countries may lack Reddit presence
- **Performance**: Bottlenecks exist in single-threaded sections (under review)

---

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë§ Author

**Berat Polat**

- GitHub: [@BrplT0](https://github.com/BrplT0)
- LinkedIn: [Berat Polat](https://www.linkedin.com/in/berat-polat-923093249)
- Email: beratpolat0402@gmail.com

---

## üôè Acknowledgments

- **Reddit & PRAW**: For comprehensive API access and excellent documentation
- **HuggingFace**: For pre-trained NLP models and transformers library
- **Streamlit Community**: For making data visualization accessible

---

## üìû Contact & Contributions

This is my first major data project, and I'm learning as I build. Feedback, suggestions, and contributions are highly appreciated!

- **Issues**: [GitHub Issues](https://github.com/BrplT0/RedditCountryHappinessAnalysis/issues)
- **Pull Requests**: Welcome! Please open an issue first to discuss major changes
- **Email**: beratpolat0402@gmail.com

---

<div align="center">

### ‚≠ê If you find this project interesting, please consider starring it!

**Status**: üöß Active Development | üìä Data Collection Complete | ü§ñ NLP In Progress

Built with ‚ù§Ô∏è and ‚òï | Learning by doing

</div>