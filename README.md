# üåç Reddit Country Happiness Analysis

> A high-throughput data pipeline that scrapes, processes, and analyzes Reddit comments to create an interactive happiness visualization dashboard.

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![PRAW](https://img.shields.io/badge/PRAW-7.7+-orange.svg)](https://praw.readthedocs.io/)
[![Transformers](https://img.shields.io/badge/ü§ó%20Transformers-4.x-yellow.svg)](https://huggingface.co/transformers/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## üìä Project Overview

This project leverages Reddit's vast user-generated content to analyze and visualize global happiness trends. By processing **hundreds of thousands of comments** weekly, we create a sentiment-based interactive dashboard showing which regions are happiest based on social media discourse.

### üéØ Current Status: **Ready for Deployment**

The complete pipeline is **production-ready**, featuring:

- üó∫Ô∏è **Interactive Choropleth Map** - Global happiness heatmap
- üìà **Time Series Analysis** - Track happiness trends over time
- üèÜ **Top/Bottom Rankings** - Happiest and unhappiest countries
- üåê **150+ Countries** - Comprehensive global coverage

---

## ‚ú® Key Features

- üöÄ **High-Throughput Scraper** - Optimized API usage with capacity of **~500,000 comments/hour**
- üéØ **Geographic Filtering** - Target specific regions or analyze global data
- ‚öôÔ∏è **Flexible Backend** - Config-switchable between CPU multiprocessing and GPU acceleration
- ü§ñ **Dual NLP Models** - Choose between XLM-RoBERTa (accurate) or DistilBERT (fast)
- üîß **Highly Configurable** - Fine-tune scraping depth, approval thresholds, and time windows
- ‚ôªÔ∏è **Smart Caching** - Re-runnable pipeline skips redundant processing
- üì¶ **Efficient Archival** - Historical data compressed and preserved

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Reddit API (PRAW)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Subreddit Checker  ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Validates 197 countries)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ    Post Scraper     ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Scrapes posts from last 7 days)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Comment Scraper    ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Capacity: ~500K/hour)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ   Preprocessing     ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (RegEx cleaning, bot/spam filtering)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Sentiment Analysis  ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (XLM-R, CPU/GPU compatible)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Data Aggregation   ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Country-level statistics)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Streamlit Dashboard‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Interactive visualization)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

### Core Pipeline
- **Python 3.12** - Core language
- **PRAW 7.7+** - Reddit API Wrapper
- **Pandas** - Data manipulation and ETL
- **NumPy** - Data chunking for multiprocessing

### NLP & Performance
- **Hugging Face `transformers`** - XLM-RoBERTa model
- **PyTorch (`torch`)** - ML backend (CPU/GPU)
- **`multiprocessing`** - Parallel processing on CPU

### Visualization
- **Streamlit** - Interactive web dashboard
- **Plotly** - Choropleth & time series visualization

### Configuration & Security
- **`configparser`** - Settings management
- **`python-dotenv`** - Environment variable handling
- **`logging`** - Multiprocessing-safe logging

---

## üìÇ Project Structure

```
RedditCountryHappinessAnalysis/
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ subreddits.csv            # üîí Populated list (Ignored by Git)
‚îÇ   ‚îî‚îÄ‚îÄ subreddits.template.csv   # ‚úÖ Public template (Tracked by Git)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.ini                # ‚úÖ Public settings (Tracked by Git)
‚îú‚îÄ‚îÄ data/                          # üîí Generated outputs (Ignored by Git)
‚îÇ   ‚îú‚îÄ‚îÄ archived/                  # Compressed historical data
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/                 # Current dashboard data
‚îÇ   ‚îú‚îÄ‚îÄ logs/                      # Pipeline execution logs
‚îÇ   ‚îú‚îÄ‚îÄ processed/                 # Cleaned and analyzed data
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Raw scraped comments
‚îÇ   ‚îî‚îÄ‚îÄ weekly_scrapings/          # Weekly scraping results
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ analyzers/                # ‚úÖ NLP and Aggregation
‚îÇ   ‚îú‚îÄ‚îÄ checkers/                 # ‚úÖ Subreddit validation
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # ‚úÖ Core utilities (Logger, Config)
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/                 # ‚úÖ Data collection
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # ‚úÖ Helper functions
‚îÇ   ‚îî‚îÄ‚îÄ dashboard/                # ‚úÖ Streamlit visualization
‚îú‚îÄ‚îÄ .env.example                  # ‚úÖ API Key template
‚îú‚îÄ‚îÄ main.py                       # Main execution pipeline
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Setup & Installation

> **Note:** The project automatically creates the `/data` directory structure on first run. You only need to configure API keys and the subreddit list.

### Prerequisites
- **Python 3.11 or 3.12** (3.12 for CPU-only, 3.11 recommended for GPU/CUDA)
- Reddit API credentials
- **16GB+ RAM** (Recommended for CPU multiprocessing on large datasets)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/BrplT0/RedditCountryHappinessAnalysis.git
cd RedditCountryHappinessAnalysis

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install core dependencies
pip install -r requirements.txt
# This installs the CPU-only version of PyTorch by default.
```

### Configuration (Required Steps)

**1. Create `.env` file (API Secrets)**

Rename `.env.example` to `.env` and add your Reddit API keys.

```bash
# On Linux/Mac:
mv .env.example .env

# On Windows:
ren .env.example .env

# Now edit the .env file with your credentials
```

**2. Create `subreddits.csv` (Input Data)**

Rename `subreddits.template.csv` to `subreddits.csv`. The template includes example subreddits.

```bash
# On Linux/Mac:
mv assets/subreddits.template.csv assets/subreddits.csv

# On Windows (CMD):
cd assets
ren subreddits.template.csv subreddits.csv
cd ..

# Edit the file if you want to add/remove subreddits
```

**3. Configure `config.ini` (Pipeline Settings)**

Open `config/config.ini` to customize the pipeline behavior. The configuration file is divided into multiple sections:

```ini
[global]
# Geographic scope: "all", "world", "asia", "africa", "europe", 
# "south_america", "north_america", "oceania"
category = europe

# Scraping lookback period (days)
comment_max_days = 7

[check_subreddits]
# Minimum comments required for subreddit approval
comment_approve_point = 1000

# Minimum subscribers required
sub_approve_point = 100000

[reddit_post_scraper]
# Max posts per subreddit
post_limit = 150

# Minimum comments per post to process
post_comment_approve_limit = 50

[reddit_comment_scraper]
# Comment depth limit (32 = optimal balance)
# 0 = top-level only, None = all (causes rate limits)
comment_link_limit = 32

[analysis]
# Hardware selection: "cpu" or "gpu"
device_type = cpu

# CPU cores (only used if device_type = cpu)
cpu_cores = 4

# Model selection: "roberta" (accurate, 1.6GB) or "distilbert" (faster, 0.5GB)
model_name = distilbert
```

> **Note:** The `/data` directory structure will be **automatically created** by the project when you first run `main.py`.

### (Optional) Setup for GPU (NVIDIA/CUDA)

If you want to use an NVIDIA GPU for faster analysis:

1. Ensure you are using **Python 3.11 or 3.12** (3.11 recommended for CUDA compatibility)
2. Uninstall the CPU-only `torch`:
   ```bash
   pip uninstall torch
   ```
3. Install CUDA-enabled PyTorch (e.g., CUDA 12.1):
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```
4. Update `config.ini`:
   ```ini
   [analysis]
   device_type = gpu
   model_name = distilbert  # Recommended for GPUs with <4GB VRAM
   ```

> **GPU Requirements:** Minimum 3GB VRAM recommended. XLM-RoBERTa requires ~1.6GB, DistilBERT requires ~0.5GB.

---

## üöÄ Usage

### Run the Complete Pipeline

```bash
python main.py
```

The script is **re-runnable**. If it finds already processed data from today (`cleaned_comments.csv`), it will skip the ~20-min scraping/cleaning steps and jump straight to the analysis.

### Run Dashboard Only

```bash
streamlit run src/dashboard/app.py
```

---

## üìà Performance Benchmarks (Local Machine)

### Scraping Performance
- **~500,000 comments/hour** with optimized API usage
- **Comment depth:** 32 levels captures 99.9% of data without rate limits
- **Geographic filtering:** Reduces processing time by targeting specific regions

### NLP Analysis Performance

**CPU Mode (Ryzen 5 8400F, 6 Cores)**
- **XLM-RoBERTa:**
  - 10,000 comments (Single-Core, `batch_size=64`): ~6 min 17 sec
  - 14,000 comments (`Pool(4)`): ~21 minutes
- **DistilBERT:** (Estimated 2-3x faster than RoBERTa)
  
> *Note: Benchmarks on Windows with small datasets. Multiprocessing overhead is significant for small volumes. Linux performance with larger datasets will show greater parallelization benefits.*

**GPU Mode**
- **NVIDIA MX350 (2GB VRAM):**
  - XLM-RoBERTa: ‚ùå `OutOfMemoryError` (model 1.6GB + batch data exceeds VRAM)
  - DistilBERT: ‚úÖ Expected to work (0.5GB model footprint)
- **Recommended GPU:** 3GB+ VRAM (e.g., T4, RTX 3060) for XLM-RoBERTa

### Model Comparison

| Model | VRAM Usage | CPU Speed | Accuracy | Recommended For |
|-------|------------|-----------|----------|-----------------|
| **XLM-RoBERTa** | ~1.6GB | Baseline | High | High-accuracy analysis, powerful GPUs |
| **DistilBERT** | ~0.5GB | 2-3x faster | Good | Production VDS, limited resources |

---

## üéì Key Learnings & Challenges

### Technical Achievements

**1. Advanced Configuration System**
- Built a comprehensive multi-section config supporting geographic filtering, threshold tuning, and hardware selection
- Implemented dual-model support (XLM-RoBERTa vs DistilBERT) for different performance/accuracy tradeoffs
- Designed optimal comment depth limit (32 levels) to capture 99.9% of data without hitting API rate limits

**2. Hardware Optimization**
- Diagnosed GPU memory limitations: 2GB VRAM insufficient for XLM-RoBERTa (1.6GB model + batch data)
- Validated DistilBERT as efficient alternative (0.5GB VRAM) for resource-constrained environments
- Optimized CPU multiprocessing as reliable path for production VDS deployment

**3. Robust Error Handling**
- `TypeError: NoneType`: Implemented `.fillna()` before processing API responses
- `IndexError: 512`: Added `truncation=True, max_length=512` to handle token limits
- `AssertionError: Torch not compiled`: Resolved CUDA vs CPU package compatibility

**4. Pipeline Efficiency**
- Idempotent design: Script checks for existing data, skips 20+ min scraping if present
- Balanced scraping strategy: `comment_link_limit=32` avoids rate limits while maximizing data capture
- Multiprocessing logging: Custom logger with `MainProcess` check prevents duplicate entries

**5. Atomic Writes for Live Dashboard**
- Solved data conflict (race condition) between the weekly pipeline (Writer) and the 7/24 dashboard (Reader). 
- Implemented shutil.move to atomically swap .tmp files, ensuring the dashboard never reads a partially written CSV and never crashes.
---

## üîÆ Roadmap

### ‚úÖ Completed
- [x] Subreddit validation system
- [x] High-throughput post & comment scraper
- [x] Preprocessing pipeline (cleaning, filtering)
- [x] Multilingual sentiment analysis (XLM-RoBERTa)
- [x] Country-level data aggregation
- [x] Interactive Streamlit dashboard with choropleth map, time series, and rankings

### üöÄ Next Steps: Deployment
- [ ] Docker containerization
- [ ] VDS deployment with automated weekly cronjob
- [ ] Production monitoring and logging

### üîú Future Enhancements
- [ ] Custom frontend development (replacing Streamlit)
- [ ] Database migration evaluation (MongoDB or similar)
- [ ] Email notification system for pipeline failures
- [ ] Advanced time series forecasting
- [ ] Multi-language UI support
- [ ] API endpoint for data access
- [ ] Mobile-responsive frontend
- [ ] Historical data comparison tools
- [ ] Sentiment trend predictions
- [ ] Regional analysis (sub-national level)

---

## üêõ Known Issues & Limitations

- **No Test Suite**: Lacks automated unit tests (e.g., `pytest`)
- **Manual Monitoring**: Production monitoring system not yet implemented
- **CSV Storage**: May need database migration for long-term scalability

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë§ Author

**Berat Polat**

- GitHub: [@BrplT0](https://github.com/BrplT0)
- LinkedIn: [Berat Polat](https://www.linkedin.com/in/berat-polat-923093249)

---

<div align="center">

### ‚≠ê If you find this project interesting, please consider starring it!

**Status**: üöÄ Ready for Deployment | üìä Pipeline Complete | ü§ñ NLP Optimized

Built with ‚ù§Ô∏è and ‚òï | Learning by doing

</div>