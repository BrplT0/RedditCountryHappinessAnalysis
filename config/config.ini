[global]

# Specifies the geographic category of subreddits to target.
# Used by 'filter_subreddit' to narrow the scope of the check.
# Valid options: "all", "world", "asia", "africa", "europe", "south_america", "north_america", "oceania"
category = world

# Sets the lookback period in days for scraping.
# '7' means scrapers will only fetch posts/comments from the last 7 days.
comment_max_days = 7

#-------------------------------------------------

[check_subreddits]

# The minimum number of comments a subreddit must have (within the 'comment_max_days' window)
# to be considered "active" and approved for scraping.
comment_approve_point = 1000

# The minimum number of subscribers a subreddit must have to be approved.
# Filters out small or inactive communities.
sub_approve_point = 100000

#-------------------------------------------------

[reddit_post_scraper]

# The maximum number of posts (e.g., from 'hot' or 'new') to retrieve
# from EACH approved subreddit during the post scraping phase.
post_limit = 10

# A filter to scrape comments only from high-engagement posts.
# A post must have at least this many comments to be processed.
post_comment_approve_limit = 50

#-------------------------------------------------

[reddit_comment_scraper]

# Configures PRAW's comment depth (the 'replace_more' limit).
# Reddit hides deep comments behind "MoreComments" links.
# - limit=0: Fetches top-level comments only (misses 90% of data).
# - limit=None (or -1): Expands ALL links (WILL hit 429 API rate limits).
# - limit=32: A balanced approach. Expands 32 levels deep, capturing 99.9% of
#             comments without triggering 429 rate limit errors.
comment_link_limit = 32

#-------------------------------------------------

[analysis]

# Configures the hardware for the heavy NLP sentiment analysis.
#
# device_type:
#   "cpu" = Uses the CPU with Multiprocessing. Slower (~1-2 hours) but reliable and works on any machine (like a VDS).
#   "gpu" = Uses an NVIDIA GPU (CUDA). Extremely fast (~10-15 min) but requires a compatible NVIDIA card
#           and the correct PyTorch (CUDA) installation (see README).
device_type = cpu

# Number of cores to use *only* if device_type is "cpu".
# Set this based on your VDS/machine specs.
# (e.g., for a 6-core VDS, '4' is safe to leave 2 cores for the OS/web).
cpu_cores = 4

# model_name:
#   Sets the specific Transformer model to be used for sentiment analysis.
#   "roberta" = XLM-RoBERTa (The default and most accurate model, but requires ~1.6GB VRAM on GPU or takes longer on CPU).
#   "distilbert" = DistilBERT-based multilingual model. (Faster, smaller (~0.5GB VRAM), and more secure on limited GPU/RAM environments).
model_name = distilbert



