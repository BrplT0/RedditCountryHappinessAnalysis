[global]

# Specifies the geographic category of subreddits to target.
# Used by 'filter_subreddit' to narrow the scope of the check.
# Valid options: "all", "world", "asia", "africa", "europe", "south_america", "north_america", "oceania"
category = europe

# Sets the lookback period in days for scraping.
# '7' means scrapers will only fetch posts/comments from the last 7 days.
comment_max_days = 7

#-------------------------------------------------

[check_subreddits]

# The minimum number of comments a subreddit must have (within the 'comment_max_days' window)
# to be considered "active" and approved for scraping.
comment_approve_point = 1000

# The minimum number of subscribers a subreddit must have to be approved.
# Filters out small or inactive communities.
sub_approve_point = 100000

#-------------------------------------------------

[reddit_post_scraper]

# The maximum number of posts (e.g., from 'hot' or 'new') to retrieve
# from EACH approved subreddit during the post scraping phase.
post_limit = 750

# A filter to scrape comments only from high-engagement posts.
# A post must have at least this many comments to be processed.
post_comment_approve_limit = 50

#-------------------------------------------------

[reddit_comment_scraper]

# Configures PRAW's comment depth (the 'replace_more' limit).
# Reddit hides deep comments behind "MoreComments" links.
# - limit=0: Fetches top-level comments only (misses 90% of data).
# - limit=None (or -1): Expands ALL links (WILL hit 429 API rate limits).
# - limit=32: A balanced approach. Expands 32 levels deep, capturing 99.9% of
#             comments without triggering 429 rate limit errors.
comment_link_limit = -1



